{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "xX3FM-nqgbSQ"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "from torchvision import datasets, models, transforms\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "from modules.dataloader import load_data, split_train_val_test\n",
    "from modules.viz import show_confusion_mat, imshow, create_grid_for_mb\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "KnJvyPRWgbWc"
   },
   "outputs": [],
   "source": [
    "class VGG(object):\n",
    "\n",
    "    def __init__(self, pretrained_model, device, num_classes=7, lr=1, reg=0.0, dtype=np.float32, mode=\"ft_extract\"):\n",
    "        self.params = {}\n",
    "        self.reg = reg\n",
    "        self.dtype = dtype \n",
    "        self.model = pretrained_model\n",
    "        self.num_classes = num_classes\n",
    "        self.lr = lr\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        self.device = device\n",
    "        self.save_model_path = '../TrashBox-VGG19_model/Model_VGG.pt'\n",
    "\n",
    "        self.set_parameter_requires_grad(mode)\n",
    "        num_features = self.model.classifier[6].in_features\n",
    "        features = list(self.model.classifier.children())[:-1]                  \n",
    "        features.extend([nn.Linear(num_features, num_classes).to(self.device)]) \n",
    "        self.model.classifier = nn.Sequential(*features)            \n",
    "                            \n",
    "    def set_parameter_requires_grad(self, mode):\n",
    "        if mode == \"ft_extract\":\n",
    "            for param in self.model.features.parameters():\n",
    "                param.requires_grad = False\n",
    "        elif mode == \"finetune_last\":\n",
    "            for param in self.model.features[:19].parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "                \n",
    "    def gather_optimizable_params(self):\n",
    "        params_to_optimize = []\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad == True:\n",
    "                params_to_optimize.append(param)\n",
    "\n",
    "        return params_to_optimize\n",
    "\n",
    "    \n",
    "    def train(self, dataloaders, dataset_sizes, num_epochs = 25):\n",
    "        best_model_wts = copy.deepcopy(self.model.state_dict())\n",
    "        best_acc = 0.0\n",
    "\n",
    "        params_to_optimize = self.gather_optimizable_params()\n",
    "        optimizer = optim.Adam(params_to_optimize, lr = self.lr)\n",
    "        exp_lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "        for epoch in tqdm(range(0, num_epochs)):\n",
    "            print(\"Epoch {}/{}\".format(epoch, num_epochs-1))\n",
    "            print('-'*10)\n",
    "            \n",
    "            for mode in ['train', 'val']:\n",
    "                if mode == \"train\":\n",
    "                    exp_lr_scheduler.step()\n",
    "                    self.model.train()\n",
    "                else:\n",
    "                    self.model.eval() \n",
    "                    \n",
    "                total_loss = 0.0\n",
    "                total_correct = 0 \n",
    "\n",
    "                for inputs, labels in dataloaders[mode]:\n",
    "                    inputs = inputs.to(self.device)\n",
    "                    labels = labels.to(self.device)\n",
    "                    \n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    with torch.set_grad_enabled(mode == 'train'):\n",
    "                        outputs = self.model(inputs)\n",
    "                        _, y_preds = torch.max(outputs, 1)\n",
    "\n",
    "                        loss = self.loss_fn(outputs, labels)\n",
    "                \n",
    "                        if mode == \"train\":\n",
    "                            loss.backward() \n",
    "                            optimizer.step()\n",
    "                \n",
    "                    total_loss += loss.item() * inputs.size(0)\n",
    "                    total_correct += torch.sum(y_preds == labels.data)\n",
    "                \n",
    "                epoch_loss = total_loss / dataset_sizes[mode]\n",
    "                epoch_acc = total_correct.double() / dataset_sizes[mode]\n",
    "\n",
    "                print('{} Loss: {:.4f} Acc: {:.4f}'.format(mode, epoch_loss, epoch_acc))\n",
    "            \n",
    "                if mode == 'val' and epoch_acc > best_acc:\n",
    "                    best_acc = epoch_acc\n",
    "                    best_model_wts = copy.deepcopy(self.model.state_dict())\n",
    "                    torch.save(self.model.state_dict(), self.save_model_path)\n",
    "                    \n",
    "            print()\n",
    "\n",
    "        print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "        self.model.load_state_dict(best_model_wts)\n",
    "\n",
    "        torch.save(self.model.state_dict(), self.save_model_path)\n",
    "        \n",
    "        return self.model\n",
    "\n",
    "\n",
    "\n",
    "    def eval_model(self, dataloaders, mode = 'val'):\n",
    "        since = time.time()\n",
    "        avg_loss, avg_acc, total_loss, total_correct = 0,0,0,0\n",
    "        num_batches = len(dataloaders[mode])\n",
    "        mode_str = \"Validation\" if mode == 'val' else \"Test\"\n",
    "        \n",
    "        print(\"Evaluating model on {} set\".format(mode_str))\n",
    "        print('-' * 10)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i, data in enumerate(dataloaders[mode]):\n",
    "                if i % 100 == 0:\n",
    "                    print(\"\\r{} batch {}/{}\".format(mode_str, i, num_batches), end='', flush=True)\n",
    "                \n",
    "                self.model.train(False)\n",
    "                self.model.eval()\n",
    "\n",
    "                inputs, labels = data\n",
    "                inputs = inputs.to(self.device)\n",
    "                labels = labels.to(self.device)\n",
    "                \n",
    "                outputs = self.model(inputs)\n",
    "                \n",
    "                _, preds = torch.max(outputs.data, 1)\n",
    "                loss = self.loss_fn(outputs, labels)\n",
    "                \n",
    "                total_loss += loss.item() * inputs.size(0)\n",
    "                total_correct += torch.sum(preds == labels.data)\n",
    "            \n",
    "                del inputs, labels, outputs, preds\n",
    "                torch.cuda.empty_cache()\n",
    "            \n",
    "        avg_loss = total_loss / dataset_sizes[mode]\n",
    "        avg_acc = total_correct.double() / dataset_sizes[mode]\n",
    "            \n",
    "        elapsed_time = time.time() - since\n",
    "        print()\n",
    "        print(\"Evaluation completed in {:.0f}m {:.0f}s\".format(elapsed_time // 60, elapsed_time % 60))\n",
    "        print(\"Average {} loss     : {:.4f}\".format(mode_str, avg_loss))\n",
    "        print(\"Average {} accuracy : {:.4f}\".format(mode_str, avg_acc))\n",
    "        print('-' * 10)\n",
    "\n",
    "                \n",
    "                \n",
    "    def load_model(self, path, train_mode = False):\n",
    "        self.model.load_state_dict(torch.load(path))\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        if train_mode == False:\n",
    "            self.model.eval()\n",
    "\n",
    "        return self.model\n",
    "\n",
    "\n",
    "    def visualize_model(self, dataloaders, num_images=16):\n",
    "        self.model.train(False)\n",
    "        self.model.eval()\n",
    "        \n",
    "        images_so_far = 0\n",
    "        file_path_base = '../TrashBox-VGG19_model/VGG'\n",
    "        confusion_matrix = torch.zeros(self.num_classes, self.num_classes)\n",
    "                                                   \n",
    "        with torch.no_grad():\n",
    "            for i, data in enumerate(dataloaders['val']):\n",
    "                inputs, labels = data\n",
    "                size = inputs.size()[0]\n",
    "                \n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                outputs = self.model(inputs)                \n",
    "                _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                for t, p in zip(labels.view(-1), preds.view(-1)):\n",
    "                    confusion_matrix[t.long(), p.long()] += 1\n",
    "                    \n",
    "                create_grid_for_mb(i, inputs, num_images, class_names, preds, labels, file_path_base)\n",
    "        show_confusion_mat(confusion_matrix, self.num_classes, class_names, outfile=file_path_base + \"confusion_matrix_vgg19.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.dataloader import load_data, split_train_val_test\n",
    "split_train_val_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "qnLbcN7qgbZB"
   },
   "outputs": [],
   "source": [
    "pathname = '../TrashBox-VGG19_model/split_dataset'\n",
    "dataloaders, dataset_sizes, class_names = load_data(pathname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/rohitmaheshwari/Documents/GitHub/TrashBox-VGG19_model'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "2zhM8t3Igbbj"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "vgg19 = models.vgg19(pretrained=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "88A6JmbUgbdy"
   },
   "outputs": [],
   "source": [
    "vgg_model = VGG(vgg19, device, num_classes=7, mode=\"finetune_all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_model.load_model('../TrashBox-VGG19_model/Model_VGG.pt', train_mode = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iEU4pkwNgbgR",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/9\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "/opt/homebrew/lib/python3.10/site-packages/PIL/Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.10/site-packages/PIL/Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.10/site-packages/PIL/Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.10/site-packages/PIL/Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: nan Acc: 0.1348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.10/site-packages/PIL/Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.10/site-packages/PIL/Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.10/site-packages/PIL/Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.10/site-packages/PIL/Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: nan Acc: 0.1342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|████████████████                                                                                                                                                 | 1/10 [1:23:00<12:27:06, 4980.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/9\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.10/site-packages/PIL/Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.10/site-packages/PIL/Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.10/site-packages/PIL/Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.10/site-packages/PIL/Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: nan Acc: 0.1353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.10/site-packages/PIL/Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.10/site-packages/PIL/Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.10/site-packages/PIL/Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.10/site-packages/PIL/Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      " 20%|████████████████████████████████▏                                                                                                                                | 2/10 [2:46:07<11:04:34, 4984.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: nan Acc: 0.1342\n",
      "\n",
      "Epoch 2/9\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.10/site-packages/PIL/Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.10/site-packages/PIL/Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.10/site-packages/PIL/Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.10/site-packages/PIL/Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "vgg_model.train(dataloaders, dataset_sizes, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "aaVbvW6Wgbi8"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../TrashBox-VGG19_model/Model_VGG'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mvgg_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../TrashBox-VGG19_model/Model_VGG\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_mode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [2], line 145\u001b[0m, in \u001b[0;36mVGG.load_model\u001b[0;34m(self, path, train_mode)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_model\u001b[39m(\u001b[38;5;28mself\u001b[39m, path, train_mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 145\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m train_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/serialization.py:699\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    696\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    697\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 699\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    701\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    702\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    703\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    704\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/serialization.py:230\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 230\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    231\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    232\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/serialization.py:211\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 211\u001b[0m     \u001b[38;5;28msuper\u001b[39m(_open_file, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../TrashBox-VGG19_model/Model_VGG'"
     ]
    }
   ],
   "source": [
    "vgg_model.load_model('../TrashBox-VGG19_model/Model_VGG.pt', train_mode = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_model_path)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "torch.save(self.model.state_dict(), self.save_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oH7C90yygblQ"
   },
   "outputs": [],
   "source": [
    "vgg_model.visualize_model(dataloaders, num_images=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q1KxufdCgbnj"
   },
   "outputs": [],
   "source": [
    "vgg_model.eval_model(dataloaders, mode = 'val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wz3Lc865gb50"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "VGG-19 model for TrashBox dataset.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
